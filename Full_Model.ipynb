{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d10c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce152c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'data/Training'\n",
    "test_dir = 'data/Testing'\n",
    "classes = os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69aff919",
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_size = 256\n",
    "crop_size = 224\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = tf.image.resize(image, [resize_size, resize_size], method=tf.image.ResizeMethod.BILINEAR) #크기 조절\n",
    "    image = tf.image.central_crop(image, central_fraction=crop_size / resize_size) #중앙 224x224\n",
    "    image = tf.math.divide(image, 255.0) #normalize\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    image = (image - mean) / std #다 normalize\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec55b36f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|██▊                                      | 27/395 [00:00<00:02, 152.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 395/395 [00:01<00:00, 225.52it/s]\n",
      "100%|████████████████████████████████████████| 105/105 [00:00<00:00, 290.17it/s]\n",
      "100%|████████████████████████████████████████| 822/822 [00:03<00:00, 240.16it/s]\n",
      "100%|████████████████████████████████████████| 115/115 [00:00<00:00, 260.98it/s]\n",
      "100%|████████████████████████████████████████| 826/826 [00:03<00:00, 244.11it/s]\n",
      "100%|████████████████████████████████████████| 100/100 [00:00<00:00, 223.33it/s]\n",
      "100%|████████████████████████████████████████| 827/827 [00:03<00:00, 223.43it/s]\n",
      "100%|██████████████████████████████████████████| 74/74 [00:00<00:00, 169.10it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = [] #Image\n",
    "y = [] #class\n",
    "for i in classes:\n",
    "    for data_dir in [train_dir, test_dir]:\n",
    "        folderPath = os.path.join(data_dir,i)\n",
    "        for j in tqdm(os.listdir(folderPath)):\n",
    "            img = cv2.imread(os.path.join(folderPath,j)) #이미지 읽기\n",
    "            img = preprocess_image(img) #전처리\n",
    "            X.append(img) #X list 넣고\n",
    "            y.append(i) # y list\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = tf.keras.utils.to_categorical([classes.index(label) for label in y]) #문자열 -> [0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fad51c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, xx, y_train, yy = train_test_split(X,y, test_size=0.2, random_state=42) #training 분할 (train,val)\n",
    "X_test, X_val, y_test, y_val = train_test_split(xx,yy, test_size=0.5, random_state=42) #training 분할 (train,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c74c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#그래프 그려주는 거 (성능 그래프)\n",
    "def plot_acc_model(acc, val_acc, epochs):\n",
    "    plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "    plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend(loc=0)\n",
    "    plt.figure()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss_model(loss, val_loss, epochs):\n",
    "    plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend(loc=0)\n",
    "    plt.figure()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24984556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sunyoung-park/miniforge3/envs/tf_gpu/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-10 13:48:01.430631: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 1/82 [..............................] - ETA: 1:38:12 - loss: 1.6115 - accuracy: 0.3125"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import VGG16, EfficientNetV2L, ResNet50\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger\n",
    "import os\n",
    "from vit_keras import vit, utils\n",
    "\n",
    "epochs=20\n",
    "\n",
    "def create_and_compile_model(base_model, output_size, model_name):\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = True\n",
    "    x = Flatten()(base_model.output)\n",
    "    output = Dense(output_size, activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model_dir = f\"model_not_pretrained/{model_name}/\"\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=100, verbose=1, restore_best_weights=True)\n",
    "    best_model_checkpoint = ModelCheckpoint(f\"{model_dir}best_model.h5\", monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "    #model_checkpoint = ModelCheckpoint(f\"{model_dir}{model_name}_epoch{{epoch}}.h5\", period=1, verbose=1)\n",
    "    csv_logger = CSVLogger(f'training_log_{model_name}.csv', separator=',', append=False)\n",
    "    return model, [early_stopping, best_model_checkpoint, csv_logger]\n",
    "\n",
    "\n",
    "base_model_vgg = VGG16(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "model_vgg, callbacks_vgg = create_and_compile_model(base_model_vgg, output_size=4, model_name='vgg')\n",
    "history_vgg = model_vgg.fit(X_train, y_train,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=(X_val, y_val),\n",
    "                            verbose=1,\n",
    "                            callbacks=callbacks_vgg)\n",
    "del base_model_vgg, model_vgg, callbacks_vgg\n",
    "\n",
    "base_model_efficientnet = EfficientNetV2L(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "model_efficientnet, callbacks_efficientnet = create_and_compile_model(base_model_efficientnet, output_size=4, model_name='eff')\n",
    "history_efficientnet = model_efficientnet.fit(X_train, y_train,\n",
    "                                              epochs=epochs,\n",
    "                                              validation_data=(X_val, y_val),\n",
    "                                              verbose=1,\n",
    "                                              callbacks=callbacks_efficientnet)\n",
    "del based_model_efficientnet, model_efficientnet, callbacks_efficientnet\n",
    "\n",
    "\n",
    "base_model_resnet = ResNet50(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "model_resnet, callbacks_resnet = create_and_compile_model(base_model_resnet, output_size=4, model_name='resnet')\n",
    "history_resnet = model_resnet.fit(X_train, y_train,\n",
    "                                  epochs=epochs,\n",
    "                                  validation_data=(X_val, y_val),\n",
    "                                  verbose=1,\n",
    "                                  callbacks=callbacks_resnet)\n",
    "del base_model_resnet, model_resnet, callbacks_resnet\n",
    "\n",
    "base_model_vit = vit.vit_b16(\n",
    "    image_size=224,\n",
    "    classes=4,\n",
    "    pretrained=True,  # 사전 학습된 가중치를 사용\n",
    "    pretrained_top=False,  # 이 부분은 특별한 경우가 아니라면 변경하지 않는 것이 좋습니다.\n",
    ")\n",
    "model_vit, callbacks_vit = create_and_compile_model(base_model_vit, output_size=4, model_name='vit')\n",
    "\n",
    "history_vit = model_vit.fit(X_train, y_train,\n",
    "                                  epochs=epochs,\n",
    "                                  validation_data=(X_val, y_val),\n",
    "                                  verbose=1,\n",
    "                                  callbacks=callbacks_vit)\n",
    "\n",
    "del model_vit, callbacks_vit, base_model_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddabc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot metrics for each model\n",
    "def plot_metrics(ax, histories, model_names, metric_name):\n",
    "    for history, model_name in zip(histories, model_names):\n",
    "        ax.plot(history.history[metric_name], label=f'{model_name} {metric_name.capitalize()}')\n",
    "        ax.plot(history.history[f'val_{metric_name}'], label=f'{model_name} Validation {metric_name.capitalize()}')\n",
    "\n",
    "    ax.set_title(f'{metric_name.capitalize()} Comparison')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(metric_name.capitalize())\n",
    "    ax.legend()\n",
    "\n",
    "# Create a list of model histories and names\n",
    "model_histories = [history_vit, history_vgg, history_efficientnet, history_resnet]\n",
    "model_names = ['ViT', 'VGG16', 'EfficientNetV2L', 'ResNet50']\n",
    "\n",
    "# Create subplots for accuracy\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plotting training and validation accuracy for each model\n",
    "plot_metrics(axs[0], model_histories, model_names, 'accuracy')\n",
    "\n",
    "# Plotting training and validation loss for each model\n",
    "plot_metrics(axs[1], model_histories, model_names, 'loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8b618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot training and validation metrics for each model\n",
    "def plot_metrics(ax, histories, model_names, metric_name, train=True, val=True):\n",
    "    for history, model_name in zip(histories, model_names):\n",
    "        if train:\n",
    "            ax.plot(history.history[metric_name], label=f'{model_name} Training {metric_name.capitalize()}')\n",
    "        if val:\n",
    "            ax.plot(history.history[f'val_{metric_name}'], label=f'{model_name} Validation {metric_name.capitalize()}')\n",
    "\n",
    "    ax.set_title(f'{metric_name.capitalize()} Comparison')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(metric_name.capitalize())\n",
    "    ax.legend()\n",
    "\n",
    "# Create a list of model histories and names\n",
    "model_histories = [history_vit, history_vgg, history_efficientnet, history_resnet]\n",
    "model_names = ['ViT', 'VGG16', 'EfficientNetV2L', 'ResNet50']\n",
    "\n",
    "# Create subplots for accuracy and loss\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plotting training accuracy for each model\n",
    "plot_metrics(axs[0, 0], model_histories, model_names, 'accuracy', train=True, val=False)\n",
    "\n",
    "# Plotting validation accuracy for each model\n",
    "plot_metrics(axs[0, 1], model_histories, model_names, 'accuracy', train=False, val=True)\n",
    "\n",
    "# Plotting training loss for each model\n",
    "plot_metrics(axs[1, 0], model_histories, model_names, 'loss', train=True, val=False)\n",
    "\n",
    "# Plotting validation loss for each model\n",
    "plot_metrics(axs[1, 1], model_histories, model_names, 'loss', train=False, val=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0beb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to plot training and validation metrics for each model\n",
    "def plot_metrics(ax, histories, model_names, metric_name, train=True, val=True):\n",
    "    for history, model_name in zip(histories, model_names):\n",
    "        if train:\n",
    "            ax.plot(history.history[metric_name], label=f'{model_name} Training {metric_name.capitalize()}')\n",
    "        if val:\n",
    "            ax.plot(history.history[f'val_{metric_name}'], label=f'{model_name} Validation {metric_name.capitalize()}')\n",
    "\n",
    "    ax.set_title(f'{metric_name.capitalize()} Comparison')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel(metric_name.capitalize())\n",
    "    ax.legend()\n",
    "\n",
    "# Create a list of model histories and names\n",
    "model_histories = [history_vit, history_vgg, history_resnet]\n",
    "model_names = ['ViT', 'VGG16', 'ResNet50']\n",
    "\n",
    "# Create subplots for accuracy and loss\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Plotting training accuracy for each model\n",
    "plot_metrics(axs[0, 0], model_histories, model_names, 'accuracy', train=True, val=False)\n",
    "\n",
    "# Plotting validation accuracy for each model\n",
    "plot_metrics(axs[0, 1], model_histories, model_names, 'accuracy', train=False, val=True)\n",
    "\n",
    "# Plotting training loss for each model\n",
    "plot_metrics(axs[1, 0], model_histories, model_names, 'loss', train=True, val=False)\n",
    "\n",
    "# Plotting validation loss for each model\n",
    "plot_metrics(axs[1, 1], model_histories, model_names, 'loss', train=False, val=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bb7472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "for model_name in [\"resnet\", \"vgg\", \"eff\"]:\n",
    "    best_model = keras.models.load_model(\"model_not_pretrained/\"+model_name+\"/best_model.h5\")\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_single_label = np.argmax(y_pred, axis=1)\n",
    "    y_test_single_label = np.argmax(y_test, axis=1)\n",
    "    precision = precision_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    recall = recall_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    f1 = f1_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    accuracy = accuracy_score(y_test_single_label, y_pred_single_label)\n",
    "    precision = round(precision, 3)\n",
    "    recall = round(recall, 3)\n",
    "    f1 = round(f1, 3)\n",
    "    accuracy = round(accuracy, 3)\n",
    "    print(str(model_name))\n",
    "    print(precision, recall, f1, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from vit_keras import vit, utils\n",
    "\n",
    "# 다른 부분은 그대로 두고, vit 모델의 경우만 수정\n",
    "for model_name in [\"vit\"]:\n",
    "    if model_name == 'vit':\n",
    "        # 모델 정의\n",
    "        base_model = vit.vit_b16(image_size=224, activation='sigmoid', pretrained=True)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        x = layers.Flatten()(base_model.output)\n",
    "        output_size = 4  # 출력 크기를 적절히 지정\n",
    "        output = layers.Dense(output_size, activation='softmax')(x)\n",
    "\n",
    "        model = models.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "        # 가중치 로딩\n",
    "        model.load_weights(\"model_not_pretrained/\"+model_name+\"/best_model.h5\")\n",
    "\n",
    "    else:\n",
    "        # 다른 모델들은 그대로 불러오기\n",
    "        model = models.load_model(\"model_not_pretrained/\"+model_name+\"/best_model.h5\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_single_label = np.argmax(y_pred, axis=1)\n",
    "    y_test_single_label = np.argmax(y_test, axis=1)\n",
    "\n",
    "    precision = precision_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    recall = recall_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    f1 = f1_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    accuracy = accuracy_score(y_test_single_label, y_pred_single_label)\n",
    "    precision = round(precision, 3)\n",
    "    recall = round(recall, 3)\n",
    "    f1 = round(f1, 3)\n",
    "    accuracy = round(accuracy, 3)\n",
    "    print(str(model_name))\n",
    "    print(precision, recall, f1, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b4abbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "for model_name in [\"resnet\", \"vgg\", \"eff\"]:\n",
    "    best_model = keras.models.load_model(\"aug02_model/\"+model_name+\"/best_model.h5\")\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_single_label = np.argmax(y_pred, axis=1)\n",
    "    y_test_single_label = np.argmax(y_test, axis=1)\n",
    "    precision = precision_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    recall = recall_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    f1 = f1_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    accuracy = accuracy_score(y_test_single_label, y_pred_single_label)\n",
    "    precision = round(precision, 3)\n",
    "    recall = round(recall, 3)\n",
    "    f1 = round(f1, 3)\n",
    "    accuracy = round(accuracy, 3)\n",
    "    print(str(model_name))\n",
    "    print(precision, recall, f1, accuracy)\n",
    "    \n",
    "from tensorflow.keras import layers, models\n",
    "from vit_keras import vit, utils\n",
    "\n",
    "# 다른 부분은 그대로 두고, vit 모델의 경우만 수정\n",
    "for model_name in [\"vit\"]:\n",
    "    if model_name == 'vit':\n",
    "        # 모델 정의\n",
    "        base_model = vit.vit_b16(image_size=224, activation='sigmoid', pretrained=True)\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "\n",
    "        x = layers.Flatten()(base_model.output)\n",
    "        output_size = 4  # 출력 크기를 적절히 지정\n",
    "        output = layers.Dense(output_size, activation='softmax')(x)\n",
    "\n",
    "        model = models.Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "        # 가중치 로딩\n",
    "        model.load_weights(\"aug02_model/\"+model_name+\"/best_model.h5\")\n",
    "\n",
    "    else:\n",
    "        # 다른 모델들은 그대로 불러오기\n",
    "        model = models.load_model(\"aug02_model/\"+model_name+\"/best_model.h5\")\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_single_label = np.argmax(y_pred, axis=1)\n",
    "    y_test_single_label = np.argmax(y_test, axis=1)\n",
    "\n",
    "    precision = precision_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    recall = recall_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    f1 = f1_score(y_test_single_label, y_pred_single_label, average='macro')\n",
    "    accuracy = accuracy_score(y_test_single_label, y_pred_single_label)\n",
    "    precision = round(precision, 3)\n",
    "    recall = round(recall, 3)\n",
    "    f1 = round(f1, 3)\n",
    "    accuracy = round(accuracy, 3)\n",
    "    print(str(model_name))\n",
    "    print(precision, recall, f1, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8e318f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_image import LimeImageExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "best_model = keras.models.load_model(\"model/\"+model_name+\"/best_model.h5\")\n",
    "explainer = LimeImageExplainer()\n",
    "image_to_explain = X_test[0]\n",
    "explanation = explainer.explain_instance(X_test[0].astype('double'), best_model.predict, top_labels=1, hide_color=0, num_samples=1000)\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=False, num_features=10, hide_rest=False)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "temp, mask = explanation.get_image_and_mask(explanation.top_labels[0], positive_only=True, num_features=5, hide_rest=True)\n",
    "plt.imshow(mark_boundaries(temp / 2 + 0.5, mask))\n",
    "shap.initjs()\n",
    "masker = shap.maskers.Image(explanation.top_labels[0], X_test[0].shape)\n",
    "explainer = shap.Explainer(best_model, masker, output_names=classes)\n",
    "explainer\n",
    "shap_values = explainer(X_test[:4], outputs=shap.Explanation.argsort.flip[:5])\n",
    "shap_values.shape\n",
    "shap.image_plot(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01921bca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
